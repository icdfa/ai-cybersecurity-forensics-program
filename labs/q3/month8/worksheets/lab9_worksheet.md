# Lab 9 Worksheet: Data Poisoning Attacks

---

## Student Name: [Your Name]

## Date: [Date]

---

## Part 1: Baseline Model Analysis

### 1.1 Clean Model Performance

Run the script with default settings to establish the baseline performance of the model trained on clean data.

- **Clean Model Accuracy:** [Fill in]
- **Clean Model F1-Score (Weighted):** [Fill in]

**Question:** Why is it important to establish a baseline with a clean model before evaluating the impact of attacks?

> [Your answer here]

---

## Part 2: Data Poisoning Attack Analysis

Run each of the data poisoning attacks and record the performance of the poisoned model in the table below. You can find the results in the `poisoning_results.json` file generated by each run.

### 2.1 Attack Comparison Table (at 10% Poison Rate)

| Attack Type              | Poisoned Model Accuracy | Performance Degradation (Accuracy) | Backdoor Success Rate (if applicable) |
| ------------------------ | ----------------------- | ---------------------------------- | ------------------------------------- |
| Label Flipping           | [Fill in]               | [Fill in]                          | N/A                                   |
| Feature Manipulation     | [Fill in]               | [Fill in]                          | N/A                                   |
| Backdoor                 | [Fill in]               | [Fill in]                          | [Fill in]                             |

**Instructions:** The "Performance Degradation" is the difference between the clean model's accuracy and the poisoned model's accuracy.

### 2.2 Analysis Questions

**Question 1:** Which data poisoning attack had the most significant impact on the model's overall accuracy? Which had the least? Explain your reasoning.

> [Your answer here]

**Question 2:** Why are backdoor attacks considered more insidious than simple availability attacks (like label flipping that just reduce accuracy)? Explain the difference in the attacker's goal and the potential impact.

> [Your answer here]

**Question 3:** Experiment with different poison rates for the label flipping attack (e.g., `--poison-rate 0.01`, `0.2`, `0.5`). Record the poisoned model's accuracy for each rate. How does the poison rate affect the success of the attack? Is there a point of diminishing returns?

- **1% Poison Rate Accuracy:** [Fill in]
- **20% Poison Rate Accuracy:** [Fill in]
- **50% Poison Rate Accuracy:** [Fill in]

> [Your analysis of the effect of poison rate here]

---

## Part 3: Defenses and Implications

### 3.1 Defense Strategies

**Question 4:** Propose two potential defense strategies against data poisoning attacks. Consider both data-level defenses (e.g., data sanitization, outlier detection) and model-level defenses (e.g., robust training methods, differential privacy).

- **Defense 1:**
  > [Your proposed defense and explanation]

- **Defense 2:**
  > [Your proposed defense and explanation]

### 3.2 Real-World Scenarios

**Question 5:** Describe a real-world scenario where a data poisoning attack could be used to compromise a security system. For example, consider a spam filter, a network intrusion detection system, or a biometric authentication system. Be specific about the attacker's goal and the potential consequences.

> [Your answer here]

---

## Submission Checklist

- [ ] Completed all sections of this worksheet.
- [ ] Attached the three `poisoning_results.json` files (one for each attack type).
- [ ] Attached the three `poisoning_comparison.png` image files.
- [ ] Included a brief summary of your key takeaways from the lab.

### Summary & Key Takeaways

> [Write your summary here]
