# Lab 12: Building an Automated AI Security Validation Pipeline

## Quarter 3, Month 9, Weeks 35-36

### Objective

This lab focuses on the critical practice of continuous security validation for AI systems. You will build an automated pipeline that regularly tests a machine learning model against a suite of known adversarial attacks. This will provide you with the skills to ensure that AI systems remain secure over time as new threats emerge.

### Learning Outcomes

Upon completion of this lab, you will be able to:

-   Design and implement an automated security testing pipeline for AI models.
-   Integrate multiple adversarial attack techniques into a single validation framework.
-   Generate and interpret security validation reports.
-   Understand the principles of continuous security monitoring for AI.

### Prerequisites

-   Completion of all previous labs in Quarter 3.
-   A strong understanding of adversarial attacks (evasion, poisoning).
-   Familiarity with CI/CD concepts is helpful but not required.

### Required Tools and Libraries

```bash
pip install tensorflow numpy scikit-learn art (Adversarial Robustness Toolbox)
```

### Part 1: Setting up the Validation Framework

**Objective:** Create a framework that can load a model and a dataset, and then run a series of security tests against it.

**Step 1: Set up the Python Script**

Create a file named `ai_security_validation.py`.

```python
import tensorflow as tf
from art.estimators.classification import TensorFlowV2Classifier
from art.attacks.evasion import FastGradientMethod
from art.utils import load_mnist

# 1. Load the dataset
(x_train, y_train), (x_test, y_test), _, _ = load_mnist()

# 2. Define and train a simple model
def create_model():
    model = tf.keras.models.Sequential([
        tf.keras.layers.Flatten(input_shape=(28, 28, 1)),
        tf.keras.layers.Dense(128, activation="relu"),
        tf.keras.layers.Dense(10, activation="softmax")
    ])
    model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])
    return model

model = create_model()
model.fit(x_train, y_train, epochs=5)

# 3. Create an ART classifier wrapper
classifier = TensorFlowV2Classifier(
    model=model,
    nb_classes=10,
    input_shape=(28, 28, 1),
    loss_object=tf.keras.losses.CategoricalCrossentropy()
)
```

### Part 2: Implementing the Security Tests

**Objective:** Implement a suite of adversarial attacks to test the model's robustness.

**Step 1: Add the Attack Functions**

```python
def run_fgsm_attack(classifier, x_test):
    print("\n--- Running FGSM Attack ---")
    attack = FastGradientMethod(estimator=classifier, eps=0.2)
    x_test_adv = attack.generate(x=x_test)
    
    # Evaluate the model on the adversarial examples
    predictions = classifier.predict(x_test_adv)
    accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)
    print(f"Accuracy on adversarial test set: {accuracy * 100:.2f}%")
    return accuracy

# You can add more attack functions here (e.g., PGD, DeepFool)

# --- Main Validation Pipeline ---
if __name__ == "__main__":
    # Evaluate the model on the clean test set
    predictions_clean = classifier.predict(x_test)
    accuracy_clean = np.sum(np.argmax(predictions_clean, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)
    print(f"Accuracy on clean test set: {accuracy_clean * 100:.2f}%")

    # Run the security tests
    fgsm_accuracy = run_fgsm_attack(classifier, x_test)

    # Generate a report
    print("\n--- AI Security Validation Report ---")
    print(f"Baseline Accuracy: {accuracy_clean * 100:.2f}%")
    print(f"Accuracy after FGSM Attack: {fgsm_accuracy * 100:.2f}%")
```

### Part 3: Automating the Pipeline

**Objective:** Simulate the automation of this pipeline, as you would in a CI/CD environment.

**Step 1: Create a Bash Script**

Create a file named `run_validation.sh`.

```bash
#!/bin/bash

echo "Starting AI Security Validation Pipeline..."

# Run the Python script
python ai_security_validation.py > validation_report.txt

echo "Validation complete. Report generated in validation_report.txt"

# You could add steps here to send the report via email or to a dashboard.
```

**Step 2: Run the Automation Script**

```bash
chmod +x run_validation.sh
./run_validation.sh
```

### Deliverables

1.  **Python Script:** Submit your complete `ai_security_validation.py` script.
2.  **Bash Script:** Submit your `run_validation.sh` script.
3.  **Validation Report:** Submit the `validation_report.txt` file generated by your pipeline.
4.  **Lab Report:** A 2-3 page report that includes:
    *   An explanation of your automated validation pipeline.
    *   The results from your validation report.
    *   A discussion on how this pipeline could be integrated into a real-world MLOps workflow.
    *   An analysis of the importance of continuous security validation for AI systems.

### Grading Rubric

| Criterion | Points | Description |
| :--- | :--- | :--- |
| Framework Setup | 25 | Correctly sets up the ART classifier and loads the data/model. |
| Attack Implementation | 35 | Successfully implements and runs the FGSM attack. |
| Automation Pipeline | 20 | Creates a functional bash script to automate the validation process. |
| Report and Analysis | 20 | A well-written report with insightful analysis and all required components. |
| **Total** | **100** | |
