# Lab 8: Bypassing a Malware Classifier with Evasion Attacks

---

## Overview

This lab provides hands-on experience with evasion attacks, a common type of adversarial attack where malicious inputs are crafted to be misclassified by a machine learning model at inference time. Students will implement several evasion techniques to bypass a pre-trained malware classifier.

## Learning Objectives

Upon completing this lab, students will be able to:

- Understand the principles of evasion attacks against machine learning models.
- Implement feature manipulation, gradient-based, and mimicry evasion attacks.
- Evaluate the effectiveness of different evasion techniques.
- Analyze the impact of adversarial perturbations on model predictions.
- Propose potential defenses against evasion attacks.

## Prerequisites

- Completion of Labs 1-7.
- Understanding of machine learning classification (Random Forest).
- Proficiency in Python and familiarity with `scikit-learn` and `pandas`.

## Lab Environment

- **Python Version:** 3.8+
- **Libraries:** `scikit-learn`, `pandas`, `numpy`, `matplotlib`
- **Setup:**
  ```bash
  # Navigate to the repository root
  cd /path/to/ai-cybersecurity-forensics-program

  # Install dependencies
  pip install -r requirements.txt
  ```

## Lab Files

- **Code:** `code/evasion_attack.py` - The main script for implementing and evaluating evasion attacks.
- **Dataset:** `data/malware_dataset.csv` - A sample dataset of malware and benign files (generated by `scripts/generate_sample_data.py`).
- **Worksheet:** `worksheets/lab8_worksheet.md` - Questions and exercises to be completed by the student.

---

## Part 1: Understanding the Malware Classifier

The first step is to understand the target model we will be attacking. The `evasion_attack.py` script trains a Random Forest classifier on a sample malware dataset. The dataset contains ten numerical features extracted from executable files and a binary label (1 for malware, 0 for benign).

### Task 1.1: Train and Evaluate the Baseline Model

1.  Navigate to the lab directory:
    ```bash
    cd labs/q3/month8/
    ```

2.  Run the script without any attack-specific arguments. This will train the classifier and evaluate its baseline performance on the test set.
    ```bash
    python3 code/evasion_attack.py
    ```

3.  **Analyze the output:**
    - What is the baseline accuracy of the classifier?
    - Review the classification report. How well does the model perform in detecting malware (recall for the 'Malware' class)?
    - The trained model is saved to `models/malware_classifier.pkl`. This is the model we will target.

---

## Part 2: Implementing Evasion Attacks

The script implements three types of evasion attacks. We will now run these attacks and analyze their effectiveness.

### Task 2.1: Feature Manipulation Attack

This is a simple attack where we add small perturbations to a few features of a malware sample to trick the classifier.

1.  Run the script with the `feature` attack type:
    ```bash
    python3 code/evasion_attack.py --attack-type feature
    ```

2.  **Analyze the output:**
    - The script attacks the first 10 malware samples from the test set.
    - For each attack, observe the change in the predicted malware probability.
    - How many attacks were successful (i.e., the malware was misclassified as benign)?
    - What is the overall success rate for this attack type?

### Task 2.2: Gradient-Based Attack

This attack uses a simplified gradient-like approach to iteratively modify the input sample in a direction that is most likely to reduce the malware prediction probability.

1.  Run the script with the `gradient` attack type:
    ```bash
    python3 code/evasion_attack.py --attack-type gradient
    ```

2.  **Analyze the output:**
    - Compare the success rate of the gradient-based attack to the feature manipulation attack.
    - Is this attack more or less effective? Why do you think that is?
    - Look at the `perturbation_magnitude` in the `attack_results.json` file. How does it compare to the feature manipulation attack?

### Task 2.3: Mimicry Attack

This attack attempts to make a malware sample look more like a benign sample by blending its features with those of a randomly selected benign file.

1.  Run the script with the `mimicry` attack type:
    ```bash
    python3 code/evasion_attack.py --attack-type mimicry
    ```

2.  **Analyze the output:**
    - How does the success rate of the mimicry attack compare to the other two?
    - This attack is often effective because it shifts the malware sample into a region of the feature space that is dense with benign samples. Explain why this makes it difficult for the classifier.

### Task 2.4: Run All Attacks

Finally, run all attacks to get a comprehensive comparison.

1.  Run the script with the default settings (`--attack-type all`):
    ```bash
    python3 code/evasion_attack.py --attack-type all
    ```

2.  **Analyze the `attack_results.json` file:**
    - This file contains detailed results for every attack performed.
    - Which attack type was the most successful overall?
    - Which attack required the smallest perturbation on average to succeed?

---

## Part 3: Analysis and Discussion

Answer the following questions in your lab worksheet (`worksheets/lab8_worksheet.md`).

1.  **Effectiveness:** Which evasion attack was the most effective against the Random Forest classifier and why do you think it performed the best?

2.  **Perturbation:** For successful attacks, what was the average perturbation magnitude? Does a successful attack always require a large change to the input?

3.  **Defenses:** Based on your observations, propose two potential defense strategies that could make the malware classifier more robust against these evasion attacks. Explain how each defense would work.

4.  **Real-World Implications:** Why are evasion attacks a significant threat to AI-powered security systems (e.g., antivirus, intrusion detection systems)? Provide a real-world example scenario.

5.  **Ethical Considerations:** What are the ethical responsibilities of researchers who discover and publish new adversarial attack techniques?

---

## Submission

- Submit your completed `worksheets/lab8_worksheet.md` file.
- Submit the `attack_results.json` file generated from running all attacks.
- Include a brief summary of your findings and key takeaways from the lab.

---

## Further Exploration (Optional)

- **Attack Different Models:** Modify the script to use a different classifier (e.g., `LogisticRegression`, `SVC`). Is it more or less vulnerable to these attacks?
- **Implement a Defense:** Try to implement one of the defenses you proposed (e.g., adversarial training, input sanitization) and evaluate its effectiveness.
- **Use a Different Dataset:** Apply these attack techniques to a different security dataset (e.g., network intrusion detection).
- **Advanced Attacks:** Research and implement more advanced evasion attacks, such as the Carlini & Wagner (C&W) attack or Projected Gradient Descent (PGD).
